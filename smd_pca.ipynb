{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Feature Extraction, Feature Generation, Dimensionsreduzierung und PCA\n",
    "\n",
    "> __In God we trust, all others bring data__\n",
    "\n",
    ">William Edwards Deming (1900-1993)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:22.200722Z",
     "start_time": "2018-11-13T16:47:22.172200Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%%javascript\n",
    "$.getScript('https://kmahelona.github.io/ipython_notebook_goodies/ipython_notebook_toc.js')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "source": [
    "<h2 id=\"tocheading\">Table of Contents</h2>\n",
    "<div id=\"toc\"></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Einleitung\n",
    "\n",
    "### Format der „Folien“\n",
    "\n",
    "Hierbei handelt es sich um ein sogenanntes Jupyter Notebook.\n",
    "\n",
    "<https://jupyter.org>\n",
    "\n",
    "Eine Mischung aus Code, Bildern und Text mit Teilweise interaktiven Elementen.\n",
    "\n",
    "Auch lokal auf euren Rechnern ausführbar. \n",
    "\n",
    "<img src=\"./ml/images/jupyter.png\" alt=\"Jupyter Logo\" style=\"width: 200px;\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### scikit-learn (sklearn)\n",
    "\n",
    "Programmbeispiele und Plots in der Vorlesung sind mithilfe des `scikit-learn` Projektes erstellt.\n",
    "\n",
    "<http://scikit-learn.org/stable>\n",
    "\n",
    "`scikit-learn` ist eine Bibliothek für Python mit vielen Methoden und Algortihmen für Data Mining / Machine Learning\n",
    "\n",
    "Ausführlicher User-Guide mit Beispielen und mathematischen Hintegründen:\n",
    "<https://scikit-learn.org/stable/user_guide.html>\n",
    "\n",
    "<img src=\"./ml/images/logo.png\" alt=\"Scikit Logo\" style=\"width: 200px;\"/>\n",
    "\n",
    "\n",
    "> Pedregosa, Fabian, et al. \"Scikit-learn: Machine learning in Python.\" the Journal of machine Learning research 12 (2011): 2825-2830."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### pandas\n",
    "\n",
    "Für die Daten-Vorverarbeitung werden wir hauptsächlich `pandas` verwenden.\n",
    "\n",
    "Pandas stellt den `DataFrame` zur Verfügung, eine tabellenartige Datenstruktur mit vielen nützlichen Methoden\n",
    "\n",
    "<https://pandas.pydata.org>\n",
    "\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ed/Pandas_logo.svg\" alt=\"pandas log\" style=\"width: 300px;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Alle benötigten Pakete sind in der `conda` Umgebung enthalten, zur Erinnerung:\n",
    " \n",
    "Umgebung erstellen:\n",
    "```\n",
    "conda env create -f environment.yml\n",
    "```\n",
    "\n",
    "\n",
    "Umgebung aktualisieren:\n",
    "```\n",
    "conda env update -f environment.yml\n",
    "```   \n",
    "\n",
    "Umgebung aktivieren\n",
    "```\n",
    "conda activate ml\n",
    "```\n",
    "\n",
    "Notebook Server starten:\n",
    "```\n",
    "jupyter notebook\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Notationen\n",
    "\n",
    "*Für weiteres siehe \"Elements of statistical Learning\" von Trevor Hastie. https://web.stanford.edu/~hastie/ElemStatLearn/ (Kostenloses E-Book)* \n",
    "\n",
    "Ich versuche folgenden Namenskonventionen zu folgen.\n",
    "\n",
    "* Großbuchstaben wie  $X$ oder $Y$ bezeichnen generische Aspekte einer Variable (i.e. die tatsächliche Zufallsvariable)\n",
    "* Beobachtungen/Realisierungen werden klein geschrieben. Die i-te Realisierung in $X$ ist $x_i$\n",
    "* Matrizen sind groß- und fettgedruckt $\\boldsymbol{X}$\n",
    "* Beobachtungen/Realisierungen sind *Zeilen* der Matrix während die beobachteten Größen in den *spalten* stehen.\n",
    "\n",
    "Wenn man beispielsweise $d=2$ Variablen, das Alter und das Gewicht von $N = 100$ Menschen misst, dann erhält man eine $N \\times d$ Matrix $\\boldsymbol{X}$.\n",
    "\n",
    "eine Beobachtung, bzw. Zeile, der Matrix wird geschrieben als $x_i = (\\mathrm{Alter}, \\mathrm{Gewicht} )$.\n",
    "\n",
    "Alle Messungen der Größe *Gewicht* sind geschrieben als  $\\boldsymbol{x}_{\\bullet 1}$, analog zum `numpy` indexing `X[:, 1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:23.953831Z",
     "start_time": "2018-11-13T16:47:22.205726Z"
    },
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "from ml import plots\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib.colors import ListedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:23.953831Z",
     "start_time": "2018-11-13T16:47:22.205726Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "\n",
    "plots.set_plot_style()\n",
    "\n",
    "\n",
    "colors = plots.colors\n",
    "cmap = plots.cmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Wiederholung\n",
    "\n",
    "Beim letzten mal:\n",
    "\n",
    "Lineare Fisher Diskriminanzanalyse:\n",
    "\n",
    "> Finde die Hyperebenen welche zwei Populationen optimal nach Fisher Kriterium trennt.\n",
    "\n",
    "Das kleine Beispiel unten zeigt wie eine Diskriminanzanalyse mit dem scikit-learn Paket durchgeführt werden kann.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=250, centers=2, random_state=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "ax.set_aspect(1)\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "\n",
    "clf = LinearDiscriminantAnalysis()\n",
    "lda = clf.fit(X, y)\n",
    "\n",
    "projection = lda.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:24.481797Z",
     "start_time": "2018-11-13T16:47:23.956854Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "fig, (ax, ax_proj) = plt.subplots(1, 2)\n",
    "\n",
    "ax.set_aspect(1, 'datalim')\n",
    "ax.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap=cmap)\n",
    "plots.draw_linear_regression_function(lda, color='gray', ax=ax)\n",
    "\n",
    "\n",
    "for label, color in zip((0, 1), cmap.colors):\n",
    "    ax_proj.hist(projection[y == label], bins=20, range=[-5, 5], color=color, histtype='step')\n",
    "\n",
    "ax_proj.axvline(0, color='gray')\n",
    "    \n",
    "None # to keep notebook output from cluttering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Probleme bei hochdimensionalen Daten:\n",
    "\n",
    "> __Curse of dimensionality (Fluch der Dimensionalität)__\n",
    ">\n",
    "> ist ein Begriff, der von Richard Bellman eingeführt wurde,   \n",
    "> um den rapiden Anstieg im Volumen beim Hinzufügen weiterer Dimensionen in einen mathematischen Raum zu beschreiben.\n",
    ">\n",
    ">[https://de.wikipedia.org/wiki/Fluch_der_Dimensionalität](https://de.wikipedia.org/wiki/Fluch_der_Dimensionalit%C3%A4t)\n",
    "\n",
    "Je höher die Dimension des Raumes, umso mehr Beobachtungen braucht man um den Raum *ausreichend* abzudecken.\n",
    "\n",
    "Im folgenden Beispiel werden 100 Punkte aus eine uniformen Verteilung zwischen 0 und 1 gezogen.  \n",
    "Darunter wird das Histogram gezeichnet.  \n",
    "Zunächst in einer Dimension und dann in zwei.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.073068Z",
     "start_time": "2018-11-13T16:47:24.486586Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "sample = rng.uniform(low=0.0, high=1.0, size=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.073068Z",
     "start_time": "2018-11-13T16:47:24.486586Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "hist, edges, plot = ax.hist(sample, range=[0, 1], bins=10, histtype='step', lw=2)\n",
    "\n",
    "ax.scatter(sample, np.full_like(sample, 1.1 * hist.max()), color='C0')\n",
    "density = np.count_nonzero(hist) / hist.size\n",
    "\n",
    "ax.set_ylabel('Häufigkeit')\n",
    "ax.set_title('Besetzte Bins: {:.2%}'.format(density))\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.897248Z",
     "start_time": "2018-11-13T16:47:25.078702Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# Gleichverteilte Zahlen in zwei Dimensionen\n",
    "sample = rng.uniform(low=0.0, high=1.0, size=(100, 2))\n",
    "\n",
    "# Ein ausführliches plotting beispiel.\n",
    "# In Zukunft werden viele Plotting-Funktionen aus dem ml modul importiert und sind nicht direkt im Notebook\n",
    "\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "\n",
    "\n",
    "# Einzelne Punkte plotten\n",
    "ax1.scatter(sample[:, 0], sample[:, 1])\n",
    "ax1.set_aspect('equal')\n",
    "\n",
    "\n",
    "# das Histogram plotten\n",
    "hist, _, _, plot = ax2.hist2d(\n",
    "    sample[:, 0],\n",
    "    sample[:, 1],\n",
    "    range=[[0, 1], [0, 1]],\n",
    "    bins=10,\n",
    "    cmap='inferno',\n",
    "    vmin=0,\n",
    ")\n",
    "\n",
    "# Anteil besetzter Bins bestimmen\n",
    "density = np.count_nonzero(hist) / hist.size\n",
    "\n",
    "ax2.set_title('Besetzte Bins: {:.0%}'.format(density))\n",
    "ax2.set_aspect('equal')\n",
    "\n",
    "fig.colorbar(plot, ax=ax2)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Im 1D Beispiel ist jeder Bin besetzt. Es gibt keine leeren bins. Im 2D Beispiel sind bereits ungefähr ein Drittel der Bins leer. \n",
    "\n",
    "Lösungsmöglichkeiten:\n",
    "\n",
    "1. Mehr Daten Speichern und höhere Kosten in Kauf nehmen.\n",
    "2. Größere Bins benutzen und die Verteilung weniger genau wiedergeben.\n",
    "3. Dimensionen reduzieren und eventuell Informationen verwerfen. \n",
    "\n",
    "Mehr Daten zu speichern ist auch heutzutage nicht immer möglich.\n",
    "\n",
    "### Beispiel IceCube\n",
    "Der IceCube Neutrino Detektor am Südpol nimmt bis zu 1 TB Daten am Tag auf.   \n",
    "Per Satelit können nur 100 GB pro Tag übetragen werden.  \n",
    "Die restlichen Daten werden einmal pro Jahr per Schiff versandt.  \n",
    "\n",
    "<img src=\"./ml/images/icecube.jpg\" alt=\"Drawing\" style=\"width: 400px;\"/>\n",
    "\n",
    "### Beispiel SKA\n",
    "Das Square Kilometer Array ist ein geplantes Radio Teleskop welches in Südafrika und Australien gebaut werden soll.\n",
    "\n",
    "Es wird aus mehreren zehntausenden Antennen bestehen.\n",
    "\n",
    "Die erwartete Datenrate liegt in der Größenordnung von mehreren  __Petabyte pro Sekunde__.\n",
    "\n",
    "Eine Speicherung ist mit heutiger Technologie völlig unmöglich.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:25.911324Z",
     "start_time": "2018-11-13T16:47:25.900962Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "IFrame('https://www.youtube.com/embed/8BBoDw2qVD0?rel=0', width=800, height=600)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hochdimensionale Daten führen aber auch zu ganz grundsätzlichen, mathematischen,  Problemen.\n",
    "Interessante Diskussion hier: \n",
    "> Why is Euclidean distance not a good metric in high dimensions?  \n",
    ">https://stats.stackexchange.com/questions/99171/why-is-euclidean-distance-not-a-good-metric-in-high-dimensions\n",
    "\n",
    "Benötigt wird also irgendeine Art von Dimensionsreduktion."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Datenreduktion\n",
    "\n",
    "Zwei grundsätzliche, kombinierbare Ansätze:\n",
    "\n",
    "1. Feature Extraction\n",
    "  * Erzeugen neuer Attribute, die Informationen mehrerer Attribute zusammenfassen\n",
    "  * Automatisches Reduzieren der Dimension durch minimieren der Varianz (Hauptkomponenteanalyse)\n",
    "   \n",
    "1. Feature Selection\n",
    "  * Verwerfen von redundanten und schwachen Attributen\n",
    "  \n",
    "  \n",
    "In der Regel werden wir in Analysen aus Rohdaten neue Attribute erzeugen und dann relevante Attribute auswählen.\n",
    "\n",
    "Dies kann in mehreren Stufen passieren, so das die Rohdaten immer weiter abstrahiert werden und Attribute näher and der physikalischen Fragestellung generiert werden."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Feature Extraction\n",
    "\n",
    "#### Beispiel 1: Einfache Datentransformation\n",
    "\n",
    "Gegeben seien Datenpunkte $X = (x_1, x_2, ...) , Y = (y_1, y_2, ...) $.\n",
    "\n",
    "Angenommen durch Fachwissen, bekannte physikalische Zusammenhänge oder einfach genaues hinsehen sei bekannt, \n",
    "dass die Daten sich besser durch Polarkoordinaten ausdrücken lassen.\n",
    "\n",
    "$$\n",
    " \\begin{align}\n",
    "     r &= x^2 + y^2 \\\\\n",
    "     \\phi &= \\operatorname{arctan2}(y, x)    \n",
    " \\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:27.177886Z",
     "start_time": "2018-11-13T16:47:25.917299Z"
    },
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "\n",
    "def transform(X):\n",
    "    r = np.sqrt(X[:, 0]**2 + X[:, 1]**2)\n",
    "    phi = np.arctan2(X[:, 1], X[:, 0])\n",
    "    return np.column_stack([r, phi])\n",
    "\n",
    "\n",
    "X_original, y = make_circles(n_samples=500, noise=0.15, factor=0.4, )\n",
    "X_transformed = transform(X_original)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:27.177886Z",
     "start_time": "2018-11-13T16:47:25.917299Z"
    },
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "# erstelle eine figure mit zwei subplots\n",
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "# plotte die ursprünglichen Punkte\n",
    "\n",
    "for X, ax, title in zip((X_original, X_transformed), axs, ('Original', 'Transformiert')):\n",
    "    \n",
    "    ax.scatter(X[:, 0], X[:, 1], c=y, cmap=cmap)\n",
    "    ax.set_xlabel('X1')\n",
    "    ax.set_ylabel('X2')\n",
    "    ax.set_title(title)\n",
    "\n",
    "axs[0].set_aspect(1, adjustable='datalim')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In diesem Beispiel is es ausreichend, allein das Attribut $r$ zu speichern um die Daten in zwei Klassen zu \n",
    "unterteilen.\n",
    "\n",
    "Die Dimensionalität reduziert sich also um die Hälfte."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "#### Beispiel 2: Datenrepresäntation\n",
    "\n",
    "Häufig sind die Daten nicht in der richtigen Darstellung für die statistischen Methoden die wir anwenden wollen.\n",
    "In diesem Beispiel sollen Textschnippsel aus dem Internet in verschiednen Kategorien eingeteilt werden.\n",
    "Dazu müssen aus den Rohdaten, den einzelnen Texten, irgendwie Variablen abgeleitet werden, die uns einen Hinweis auf die Kategorie geben können.\n",
    "\n",
    "Wir laden zunächst einen Beispieldatensatz mit Texten aus zwei Kategorien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.100707Z",
     "start_time": "2018-11-13T16:47:27.181136Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "atheist_texts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), categories=['alt.atheism'])\n",
    "religous_texts = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'), categories=['talk.religion.misc'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.100707Z",
     "start_time": "2018-11-13T16:47:27.181136Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.100707Z",
     "start_time": "2018-11-13T16:47:27.181136Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Aus dem Atheisten Forum:\\n')\n",
    "print(rng.choice(atheist_texts.data).lstrip())\n",
    "print('\\n'*3)\n",
    "\n",
    "print('Aus dem Religions Forum:\\n')\n",
    "print(rng.choice(religous_texts.data).lstrip())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothese 1__: Atheisten benutzen mehr Wörter\n",
    "\n",
    "Wir füllen die Längen der Texte in normierte Histogramme ein um zu sehen, ob sich die Verteilungen unterscheiden."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:31.680634Z",
     "start_time": "2018-11-13T16:47:31.103985Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def number_of_words(text: str):\n",
    "    return len(text.split())\n",
    "\n",
    "atheist_lengths = list(map(number_of_words, atheist_texts.data))\n",
    "religous_lengths = list(map(number_of_words, religous_texts.data))\n",
    "\n",
    "\n",
    "bins =  np.arange(0, 2000, 25)\n",
    "\n",
    "hist_options = dict(\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    "    histtype='step',\n",
    "    lw=4,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(atheist_lengths,  label='Atheist', **hist_options)\n",
    "plt.hist(religous_lengths, label='Religous', **hist_options)\n",
    "plt.xlabel('Text Länge')\n",
    "plt.legend()\n",
    "None\n",
    "\n",
    "print(f'Median     : {np.median(atheist_lengths):.0f} {np.median(religous_lengths):.0f}')\n",
    "print(f'75%-Quantil: {np.percentile(atheist_lengths, 75):.0f}, {np.percentile(religous_lengths, 75):.0f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keine signifikanten Unterschiede per Auge zu sehen.\n",
    "\n",
    "__Hypothese 2__: Atheisten benutzen längere Wörter\n",
    "\n",
    "Selbes vorgehen wie oben"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:32.289809Z",
     "start_time": "2018-11-13T16:47:31.683907Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def max_word_length(s):\n",
    "    words = s.split()\n",
    "    if not words:\n",
    "        return 0\n",
    "    return max(map(len, words))\n",
    "\n",
    "atheist_lengths = list(map(max_word_length, atheist_texts.data))\n",
    "religous_lengths = list(map(max_word_length, religous_texts.data))\n",
    "\n",
    "\n",
    "# Histogrammieren der Wortlängen. \n",
    "bins =  np.arange(0, 100, 1)\n",
    "hist_options = dict(\n",
    "    bins=bins,\n",
    "    alpha=0.5,\n",
    "    density=True,\n",
    "    histtype='step',\n",
    "    lw=4,\n",
    ")\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(atheist_lengths, label='Atheists', **hist_options)\n",
    "plt.hist(religous_lengths, label='Religous', **hist_options)\n",
    "plt.legend()\n",
    "None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wo kommen die großen Wortlängen her? Stichwort __Datenbereinigung__. Manche der Texte Enthalten Signaturen oder andere Zeichenfolgen die nicht zum Haupttext gehören.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:32.306869Z",
     "start_time": "2018-11-13T16:47:32.294360Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "atheist_texts.data[2].split()[40:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Hypothese 3__: Atheisten benutzen andere Wörter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:33.752517Z",
     "start_time": "2018-11-13T16:47:32.311675Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from collections import Counter\n",
    "\n",
    "\n",
    "def extract_words(texts):\n",
    "    return (\n",
    "        ' '.join(texts.data)           # join all texts to one giant string\n",
    "        .lower()                       # all to lower case\n",
    "        .translate(string.punctuation) # remove punctuation ,.- etc.\n",
    "        .split()                       # split sring into list of words at whitespace\n",
    "    )\n",
    "\n",
    "\n",
    "def most_common(words, n, min_length=5):\n",
    "    counter = Counter(filter(lambda w: len(w) > min_length, words))\n",
    "    return dict(counter.most_common(n))\n",
    "\n",
    "\n",
    "atheist_words = extract_words(atheist_texts)\n",
    "common_atheist_words = most_common(atheist_words, 15)\n",
    "\n",
    "religous_words = extract_words(religous_texts)\n",
    "common_religous_words = most_common(religous_words, 15)\n",
    "\n",
    "\n",
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.set_title('Atheists')\n",
    "ax1.barh(list(common_atheist_words.keys()), list(common_atheist_words.values()))\n",
    "\n",
    "ax2.set_title('Religous')\n",
    "ax2.barh(list(common_religous_words.keys()), list(common_religous_words.values()), color='C1')\n",
    "\n",
    "\n",
    "atheist_set = set(common_atheist_words.values())       \n",
    "religous_set = set(common_religous_words.values())\n",
    "s = atheist_set | religous_set\n",
    "print(*(atheist_set - religous_set))\n",
    "print(*(religous_set - atheist_set))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Die benutzten Wörter unterscheiden sich also deutlich. Die Anzahl der Wörter die man benutzen kann um die Kategorie über die Wortfrequenz zuzuordnen ist in diesem Beispiel relativ klein. \n",
    "\n",
    "\n",
    "In echten Problemen ist das natürlich wesentlich aufwendiger und man benötigt andere Techniken und mehr Beispiele.\n",
    "\n",
    "\n",
    "Diese Art der Feature Extraction erfordert irgendeine Art von Expertenwissen und wird bei steigender Dimensionalität komplizierter bis unmöglich. \n",
    "\n",
    "Expertenwissen bedeutet häufig, dass bekannte physikalische Zusammenhänge ausgenutzt werden. Manchmal sind es aber gerade diese physikalischen Zusammenhänge die unbekannt sind und gelernt werden sollen. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### PCA (Hauptkomponentenanalyse)\n",
    "\n",
    "Die Hauptkomponentenanalyse sucht nach einer Basis im Raum indem die Varianz entlang der Basisvektoren maximiert wird.\n",
    "\n",
    "Gegeben seien also $N$ Datenpunkte mit $d$ Dimensionen, die auf $k \\leq d$ Dimensionen transformiert werden sollen.\n",
    "\n",
    "Dazu wird der Raum in eine neue Basis transformiert. Es werden also eventuell mehrere Dimensionen/Attribute zu einer neuen zusammengefasst.  \n",
    "\n",
    "Grober Ablauf der PCA\n",
    "\n",
    "0. Zentriere die Daten auf ihren Mittelwert.\n",
    "1. Berechne die Kovarianzmatrix aus der Datenmatrix $\\boldsymbol{X}$\n",
    "2. Berechne Eigenwerte und Eigenvektoren der Matrix\n",
    "3. Wähle die $k$ größten Eigenwerte und zugehörigen Eigenvektoren aus. \n",
    "4. Bilde eine $d \\times k$ Matrix $\\boldsymbol{W}$ mit den $k$ Eigenvektoren als Spalten.\n",
    "5. Wende $\\boldsymbol{W}$ auf jede Zeile aus $x$ aus $\\boldsymbol{X}$ an $x^\\prime = \\boldsymbol{W}^T \\cdot x^T $ \n",
    "    \n",
    "\n",
    "###### 1. Zentrierung \n",
    "\n",
    "Berechne die Mittelwertvektoren $\\mu$:\n",
    "\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\begin{pmatrix}\n",
    "    \\bar{\\boldsymbol{x}}_1 \\\\\n",
    "    \\ldots \\\\\n",
    "    \\bar{\\boldsymbol{x}}_d \\\\\n",
    "\\end{pmatrix}\n",
    " = \\frac 1 N\n",
    " \\begin{pmatrix}\n",
    "    \\sum_{i=0}^{N} \\boldsymbol{x}_{1, i} \\\\\n",
    "    \\ldots \\\\\n",
    "    \\sum_{i=0}^{N} \\boldsymbol{x}_{d, i} \\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Anders ausgedrückt:\n",
    "$$\n",
    "\\boldsymbol{\\mu} = \\begin{pmatrix}\n",
    "    \\text{Mittelwert aller Beobachtungen von Attribut 1}\\\\\n",
    "    \\ldots \\\\\n",
    "    \\text{Mittelwert aller Beobachtungen von Attribut d}\\\\\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "Neuer Datenpunkt:\n",
    "\n",
    "   $$\n",
    "      x^{\\prime}_i = x_i - \\boldsymbol{\\mu}\n",
    "   $$\n",
    "\n",
    "###### 2. Kovarianz\n",
    "\n",
    "Die Kovarianz einer Zufallsvariable $X$ beliebiger Dimension.\n",
    "$$\n",
    "\\operatorname {Cov} (X)=\\operatorname {E} {\\bigl [}(X-\\operatorname {E} (X))\\cdot (X-\\operatorname {E} (X))^T{\\bigr ]}\n",
    "$$\n",
    "\n",
    "Schätzung der Kovarianzmatrix auf Daten auch durch *einfache* Matrixoperation möglich. \n",
    "\n",
    "###### 3. Eigenwerte und Vektoren\n",
    "\n",
    "Berechne die $d$ verschiedenen Eigenwerte der Kovarianzmatrix $ \\operatorname {Cov} (\\boldsymbol {X} )$.\n",
    "\n",
    "Erhalte Eigenwerte $\\lambda_1, \\ldots, \\lambda_d$ mit passenden Eigenvektoren $v_1, \\ldots, v_d$\n",
    "\n",
    "\n",
    "###### 4. Sortierung und Auswahl\n",
    "\n",
    "Sortiere die Indizes der Eigenwerte und Vektoren so dass gilt \n",
    "\n",
    "$$\n",
    "\\lambda_1 > \\lambda_2 > \\lambda_3 \\ldots > \\lambda_d\n",
    "$$\n",
    "\n",
    "Wähle die $k$ größten Eigenwerte aus und verwerfe alle anderen Eigenwerte und Vektoren.\n",
    "\n",
    "\n",
    "\n",
    "###### 5. Bildung der Matrix\n",
    "\n",
    "Nutze die $k$ ausgewählten Eigenvektoren als Spalten in der Matrix $\\boldsymbol{W}$\n",
    "\n",
    "$$\n",
    "\\boldsymbol{W} = \\begin{pmatrix}\n",
    "    v_1, \n",
    "    \\ldots,  \n",
    "    v_k\n",
    "\\end{pmatrix}\n",
    "= \\begin{pmatrix}\n",
    "    v_{1,1}, \n",
    "    \\ldots,  \n",
    "    v_{k, 1} \\\\\n",
    "    \\ldots \\\\\n",
    "        v_{1,d}, \n",
    "    \\ldots,  \n",
    "    v_{k, d}\n",
    "\\end{pmatrix}\n",
    "$$\n",
    "\n",
    "###### 6. Transformierung \n",
    "\n",
    "Multipliziere die Transformationsmatrix $\\boldsymbol{W}$ mit jeder Beobachtung $x_i$ in $\\boldsymbol{X}$ um die auf $k$ Dimensionen beschränkten Punkte zu erhalten.\n",
    "\n",
    "$$\n",
    "\\boldsymbol{X^\\prime} = \\boldsymbol{X} \\boldsymbol{W}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:33.911709Z",
     "start_time": "2018-11-13T16:47:33.755670Z"
    }
   },
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)\n",
    "\n",
    "N = 8 # anzahl beobachtungen\n",
    "d = 4 # anzahl variablen/ dimensionen\n",
    "\n",
    "X = rng.normal(size=(N, d))\n",
    "X = X - X.mean(axis=0)\n",
    "\n",
    "# print(X.shape)\n",
    "c = np.cov(X, rowvar=False)\n",
    "\n",
    "# eigh garantiert sortierte Eigenwerte, allerdings aufsteigend\n",
    "l, W = np.linalg.eigh(c)\n",
    "\n",
    "# Reihenfolge umkehren. Größte Eigenwerte zuerst.\n",
    "l = l[::-1]\n",
    "W = W[:, ::-1]\n",
    "\n",
    "X_prime = X @ W"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:33.911709Z",
     "start_time": "2018-11-13T16:47:33.755670Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_prime_sklearn = pca.fit_transform(X)\n",
    "\n",
    "\n",
    "with np.printoptions(precision=2):\n",
    "    print('Per Hand:\\n', X_prime)\n",
    "    print('sklearn:\\n', X_prime_sklearn)\n",
    "\n",
    "# testen aller einträge auf gleichheit (bis auf vorzeichen)\n",
    "print('\\n All close:', np.allclose(np.abs(X_prime), np.abs(X_prime_sklearn)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### Beispiel in 3D\n",
    "\n",
    "Künstlicher Datensatz mit $d = 3$ Dimensionen wird auf $k=2$ Dimensionen reduziert. \n",
    "Der Datensatz wird gezogen aus zwei Gaussverteilungen mit unterschiedlichen Mittelwerten und gleicher Kovarianzmatrix.\n",
    "Die Darstellung unten Zeigt die Punktwolke aus vier Verschiedenen Richtungen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:35.613041Z",
     "start_time": "2018-11-13T16:47:33.914685Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=500, n_features=3, cluster_std=3, random_state=2)\n",
    "plots.plot_3d_views(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:37.396225Z",
     "start_time": "2018-11-13T16:47:35.616379Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=3)\n",
    "transformed = pca.fit_transform(X)\n",
    "\n",
    "plots.plot_3d_views(transformed, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:38.050103Z",
     "start_time": "2018-11-13T16:47:37.399569Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "transformed = pca.fit_transform(X)\n",
    "\n",
    "plt.figure()\n",
    "plt.scatter(transformed[:, 0], transformed[:, 1], c=y, cmap=cmap)\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel Gesichtserkennung \n",
    "\n",
    "Man betrachte jeden einzelnen Pixel eines Bildes als Attribut dessen Wert der Grauwert des Pixels ist.\n",
    "\n",
    "So wird ein Bild zu einem 1D Vektor. Mehrere Bilder ergeben wieder die Datenmatrix $\\boldsymbol{X}$\n",
    "\n",
    "Ein Bild mit $64 \\times 64$ Pixeln wird so zu einem Vektor der Länge $4096$.\n",
    "\n",
    "##### Ein einfacher Gesichtserkennungsalgorithmus\n",
    "\n",
    "Angenommen die Aufgabe wäre die Zuordnung von Fotos aller Studierenden der TU zu deren Namen.   \n",
    "Gesucht wird also eine Funktion die aus einem Foto einen Namen macht. \n",
    "\n",
    "Idee:\n",
    "1. Speichere Bilder aller Studierenden in einer Matrix $\\boldsymbol{X}$ der Dimension $\\text{Anzahl Studierende} \\times \\text{Anzahl Pixel}$ und einen Labelvektor $y$ der Länge $N$ der die Namen (oder Matrikelnummern) enthält.\n",
    "2. Berechne die Distanz $D$ zwischen einem neuen Foto $x_{\\text{neu}}$ zu allen in $\\boldsymbol{X}$ gespeicherten Bildern. \n",
    "3. Gebe zurück das $y_i$ für das $i$ bei dem $D(x_{\\text{neu}}, x_i)$ minimal ist.\n",
    "\n",
    "Probleme: \n",
    " - Alle Bilder zu speichern ist schwierig bis unmöglich. \n",
    " - Die Distanz zu allen Einträgen zu finden kann zu lange dauern.\n",
    " - Wahl des Distanzmaßes in hohen Dimensionen ist nicht trivial.\n",
    " \n",
    "##### Eigenfaces \n",
    "\n",
    "*Original Artikel von 1991 von Turk und Pentland http://www.mitpressjournals.org/doi/10.1162/jocn.1991.3.1.71*\n",
    "\n",
    "Der Input in den Algorithmus ist der selbe wie oben. Die Matrix aller Fotos $\\boldsymbol{X}$. Diesmal wird diese Matrix jedoch nicht komplett abgespeichert.\n",
    "\n",
    "Idee:\n",
    "1. Wende PCA auf $\\boldsymbol{X}$ an. \n",
    "2. Erhalte Transformationsmatrix $\\boldsymbol{W}$ der Dimension $d \\times k$\n",
    "3. Berechne Gewichte $g_m = \\boldsymbol{v}_m^T \\cdot(x_i - \\boldsymbol{\\mu}) $ für jedes gespeicherte Bild $x_i$ und jeden Eigenvektor $\\boldsymbol{v}_m$ mit $m \\in \\{1, \\ldots, k\\}$ und erhalte so einen Gewichtsvektor $G$ der länge $k$.\n",
    "4. Berechne Distanz $D$ zwischen dem Gewichtsvektor eines neuen Bildes $G_{\\text{neu}}$ zur allen Gewichtsvektoren in der alten Bilder aus. \n",
    "5. Gebe zurück das $y_i$ für das $i$ bei dem $D(G_{\\text{neu}}, G^{i})$ minimal ist.\n",
    "\n",
    "In der Realität ist die Berechnung der PCA auf großen Matrizen nicht immer Trivial.\n",
    "\n",
    "###### Python Beispiel für Eigenfaces\n",
    "\n",
    "Der LFW (Labeled Faces in the Wild) Datensatz ist ein beliebter Datesatz für Algorithmen zur Gesichtserkennung.\n",
    "http://vis-www.cs.umass.edu/lfw/\n",
    "Er enthält etwa 13.000 Bilder von mehreren hundert Personen die aus dem Internet heruntergeladen wurden.\n",
    "\n",
    "Für dieses Beispiel gucken wir uns nur 200 der Bilder an um Rechenzeit zu sparen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:39.027898Z",
     "start_time": "2018-11-13T16:47:38.055271Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_lfw_people\n",
    "\n",
    "lfw_people = fetch_lfw_people(min_faces_per_person=10, resize=0.8)\n",
    "\n",
    "X = lfw_people.data\n",
    "y = lfw_people.target\n",
    "names = lfw_people.target_names\n",
    "\n",
    "X.shape, y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lfw_people.target_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:39.027898Z",
     "start_time": "2018-11-13T16:47:38.055271Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "index = 15\n",
    "\n",
    "h, w = lfw_people.images[0].shape\n",
    "img = X[index].reshape(h, w)\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(img, cmap='gray')\n",
    "plt.title(rf'Bild {index} aus $\\mathbf{{\\mathit{{X}}}}$ ({names[y[index]]})')\n",
    "None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:39.598180Z",
     "start_time": "2018-11-13T16:47:39.031942Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "mean_face = X.mean(axis=0).reshape(h, w)\n",
    "plt.figure()\n",
    "plt.imshow(mean_face, cmap='gray')\n",
    "plt.title('Durschnittliches Gesicht')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:40.141860Z",
     "start_time": "2018-11-13T16:47:39.601653Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "n_components = 100\n",
    "pca = PCA(n_components=n_components)\n",
    "pca.fit(X)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:41.150702Z",
     "start_time": "2018-11-13T16:47:40.147771Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "f, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.imshow(eigenfaces[0], cmap='gray')\n",
    "ax1.set_title('Eigenface zu Eigenwert 0')\n",
    "\n",
    "ax2.imshow(eigenfaces[n_components - 2], cmap='gray')\n",
    "ax2.set_title(f'Eigenface zu Eigenwert {n_components - 2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Die Gesichtserkennung\n",
    "\n",
    "\n",
    "Wir nehmen ein Testbild aus dem Datensatz und gucken, ob wir ein zugehöriges Gesicht finden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = np.random.default_rng(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "index = rng.choice(len(X))\n",
    "test_img = X[index]\n",
    "test_name = names[y[index]]\n",
    "\n",
    "X_rest = np.delete(X, index, axis=0)\n",
    "y_rest = np.delete(y, index)\n",
    "\n",
    "plt.figure()\n",
    "plt.title(f'Bild {index} ({test_name})')\n",
    "plt.imshow(test_img.reshape(h, w), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for this large an input, sklearn uses a randomized pca by default\n",
    "# make sure to use the normal one we introduced above (slower)\n",
    "pca = PCA(n_components=n_components, svd_solver='full')\n",
    "\n",
    "X_trafo = pca.fit_transform(X_rest)\n",
    "test_trafo = pca.transform(test_img[np.newaxis, :])\n",
    "\n",
    "best_match = np.argmin(np.linalg.norm(X_trafo - test_trafo, axis=1))\n",
    "\n",
    "prediction = names[y_rest[best_match]]\n",
    "\n",
    "plt.figure()\n",
    "plt.imshow(X_rest[best_match].reshape(h, w), cmap='gray')\n",
    "plt.title(f'Best Match: {best_match} ({prediction})')\n",
    "\n",
    "None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Nachteile der PCA\n",
    "\n",
    "Intepretierbarkeit?\n",
    "\n",
    "Gerade in physikalischen Problemstellungen schwer nachvollziehbare neue Features.\n",
    "\n",
    "\n",
    "Annahme: wir messen Energie $E$, Zeit $t$ und Koordinaten $x$ und $y$?\n",
    "\n",
    "* Was bedeutet eine Hauptkomponente die sich zu\n",
    "  $$\n",
    "    0.78 \\cdot E - 0.23 \\cdot t + 0.8 \\cdot x - 0.2 \\cdot y\n",
    "  $$\n",
    "  berechnet?\n",
    "\n",
    "* Einheiten?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Selection \n",
    "\n",
    "Eine anderer Ansatz zur Dimensionsreduzierung. Dabei werden die Daten nicht Transformiert sondern einfach Spalten entfernt die keine/wenig Aussagekraft besitzen. Die Aussagekraft bezieht dabei auf ein zu Lösendes Klassifizierungsproblem. Welche Spalten man verwerfen kann, ist also sehr Problemspezifisch. Im Allgemeinen folgen viele Feature Selection Ansätze der Heuristik:\n",
    "\n",
    "> Good feature subsets contain features highly correlated with the classification, yet uncorrelated to each other.\n",
    "> \n",
    "> -- Mark Hall\n",
    "\n",
    "### Univariate Feature Selection\n",
    "\n",
    "Betrachtet jedes Attribut für sich alleinstehend. Häufig beinhaltet das auch eine Art der händischen Vorverarbeitung.\n",
    "\n",
    "#### Korrelation mit der Zielgröße\n",
    "\n",
    "Angenommen man wolle eine Größe $y$ aus einem 4D Datensatz schätzen. Ein Teil dieser Daten sind aber mit hohem Rauschen versehen oder haben mit der Zielgröße keinen Kausalen oder Statistischen Zusammenhang.\n",
    "\n",
    "Ein einfacher Algorithmus sucht einfach nach den $k$ Attributen mit denhöchsten Korrelationen und entfernt alle anderen.\n",
    "\n",
    "In dem Beispiel unten wird ein Datensatz erzeugt in welchem nur zwei der 4 vorhandenen Attribute mit der Zielgröße $y$ korrelieren. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:42.531810Z",
     "start_time": "2018-11-13T16:47:41.154671Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_regression\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "X, y = make_regression(n_samples=300, n_features=4, n_informative=2, n_targets=1, random_state=0, noise=0.1)\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize=(9, 9), sharex=True, sharey=True)\n",
    "\n",
    "for i, (ax, col) in enumerate(zip(axs.flat, X.T)):\n",
    "    ax.scatter(col, y)\n",
    "    ax.set_ylabel('Target Variable')\n",
    "    ax.set_xlabel('X{}'.format(i))\n",
    "    \n",
    "    r, _ = pearsonr(X[:, i], y)\n",
    "    ax.set_title(rf'Korrelation $\\rho = {r:.3f}$')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multivariate Feature Selection\n",
    "\n",
    "Testet Kombinationen und ganze Untermengen aus Attributen nach verschiedenen Kriterien. Zum Beispiel Korrelation, Mutual Information, Kreuzentropie, Minimal Desciption Length oder einfach die Qualität der Klassifikation/Trennung. \n",
    "\n",
    "\n",
    "Es ist im allgemeinen nicht möglich alle Kombinationen von Attributen zu testen. Die Anzahl der möglichen Kombinationen ist exponentiell. Bei $n$ Attributen müssen nach Binomischen Lehrsatz Kombinationen getestet werden:\n",
    "\n",
    "$$\n",
    "N = \\sum_{k = 1}^{n} \\begin{pmatrix}\n",
    "    n\\\\\n",
    "    k\\\\\n",
    "    \\end{pmatrix} = 2^n -1\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#### Miteinander Korrelierte Attribute \n",
    "\n",
    "In dem Beispiel unten sieht man schnell, dass zwei Attribute miteinander korrelieren. Eines der beiden Attribute ist also überflüssig bzw. redundant. \n",
    "\n",
    "Um miteinander korrelierte Attribute zu finden kann man einfach alle Paare von Attributen miteinander verlgeichen. Der verlgeich von allen Paaren miteinander führt zur quadratischen Laufzeit dieser Methode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-13T16:47:44.534634Z",
     "start_time": "2018-11-13T16:47:42.537230Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_classification\n",
    "from scipy.stats import pearsonr\n",
    "from itertools import combinations\n",
    "\n",
    "X, y = make_classification(n_samples=300, n_features=3, n_informative=2, n_redundant=1, n_repeated=0, n_classes=2, n_clusters_per_class=2, random_state=0)\n",
    "plots.plot_3d_views(X, y)\n",
    "\n",
    "for i, j in combinations(range(3), 2):\n",
    "    r, p = pearsonr(X[:, i], X[:, j])\n",
    "    print('Korrelation zwischen Attribut {} und {} : {:.2f}'.format(i + 1, j + 1, r))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es gibt Selektionsstrategien und Heuristiken um weniger Kombinitionen zu testen. Die meisten beschränken sich darauf nur Paare von Kombinationen zu testen. \n",
    "Diese art von Heuristik wird deshalb häufig auch als Bivariat bezeichnet.\n",
    "\n",
    "Die beiden einfachsten oder bekanntesten Stretegien sind Forward beziehungsweise Backward Selection\n",
    "\n",
    "\n",
    "###### Forward Selection:\n",
    "\n",
    "Das Verfahren arbeitet iterativ und testet Untermengen nach einem festzulegenden Kriterium.\n",
    "Starte mit dem einzelnen besten Attribut $f_0$ und füge so lange Attribute hinzu bis ein Abbruchkriterium erreicht ist. \n",
    "\n",
    "###### Backward Selection:\n",
    "\n",
    "Wie Forward Selection aber es wird von der vollen Menge an Attributen gestartet und dann iterativ Einträge entfernt.\n",
    "\n",
    "#### Max-Relevance, Min-Redundancy (mRMR)\n",
    "\n",
    "Original veröffentlichung von Peng et al. (2005): [ieeexplore.ieee.org/document/1453511/](ieeexplore.ieee.org/document/1453511/)\n",
    "\n",
    "Wähle die Untermenge an Attributen $S_k = \\{f_1, f_2, \\ldots, f_k\\}$ die Insgesamt die höchste Relevanz bezüglich der Zielvariable $y$ hat und gleichzeitig die Korrelation zwischen den Attributen in $S_k$ möglichst klein ist.\n",
    "\n",
    "Die Relevanz wird häufig durch ein Korrelationsmaß oder die sogenannte Mutual Information bestimmt.\n",
    "Mehr zum Thema Mutual Information folgt noch. \n",
    "\n",
    "Für das gesuchte $S_k$ soll gelten $\\max _{S_{k}}(D - R)$, wobei\n",
    "\n",
    "\\begin{align}\n",
    "D(S, y) =& {\\frac {1}{|S|}}\\sum _{f_{i}\\in S}I(f_{i}, y) \\\\\n",
    "R(S)   =& {\\frac {1}{|S|^{2}}}\\sum _{f_{i},f_{j}\\in S}I(f_{i}, f_{j}) \n",
    "\\end{align}\n",
    "\n",
    "Die Untermengen werden gebildet wie bei Forward Selection. Das nächste Attribut wird ausgewählt nach \n",
    "\n",
    "$$\n",
    "\\mathrm {mRMR} =\\max _{S}\\left[{\\frac {1}{|S|}}\\sum _{f_{i}\\in S}I(f_{i}, y)-{\\frac {1}{|S|^{2}}}\\sum _{f_{i},f_{j}\\in S}I(f_{i}, f_{j})\\right].\n",
    "$$\n",
    "\n",
    "Der mRMR Algorithmus gehört zu einer Klasse von Algorithmen die versuchen Relevanz zu maximieren und Redundanz zu minimieren. Er hat einige interessante Eigenschaften aus informationstheoretischer Sicht und ist vor allem in Anwendungen der Biologie und Genetik interessant.\n",
    "\n",
    "\n",
    "**Feature Selection ist vor allem dann Wichtig wenn die Anzahl der Attribute größer ist als die Anzahl der Beispiele im Datensatz.**\n",
    "\n",
    "### Probleme\n",
    "\n",
    "Algorithmen wie mRMR die sich auf iterative Auswahlen verlassen, indem sie zum Beispiel Forward Selection benutzen, werden auch \"greedy\" Heuristiken genannt.\n",
    "\n",
    "Es ist nicht immer gewährleistet, dass auch das globale Optimum erreicht wird wenn eine greedy Heuristik benutzt wird.\n",
    "\n",
    "Das hängt natürlich von der zu optimierenden Zielfunktion ab.\n",
    "\n",
    "Im allgemeinen ist es schwierig bzw. unmöglich in annehmbarer Zeit die \"optimale\" Untermenge an Attributen zu finden.\n",
    "\n",
    "Interresanter Artikel:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Multivariate_mutual_information\n",
    "\n",
    "\n",
    "<p style=\"color:gray\"> Für die Theoretiker: Das VERTEX-COVER Problem kann auf MIN_FEATURE reduziert werden. Dadurch wird es NP-Vollständig. http://scottdavies.net/aaai94.pdf </p> \n",
    "\n",
    "\n",
    "Alle hier angeführten Algorithmen gehen davon aus, dass ungeeignete Attribute schon entfernt wurden, z.B.:\n",
    "\n",
    "* Simulations-Attribute\n",
    "* Attribute mit großen Unterschieden zwischen gemessenen und simulierten Daten"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
